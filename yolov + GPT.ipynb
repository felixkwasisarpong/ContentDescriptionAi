{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19723,"status":"ok","timestamp":1722723989444,"user":{"displayName":"felix sarpong","userId":"04486120806717242160"},"user_tz":300},"id":"S1l_E3aMy9rZ","outputId":"e41441c2-6d22-44b6-fb69-bf0d0593d613"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":48775,"status":"ok","timestamp":1722724038214,"user":{"displayName":"felix sarpong","userId":"04486120806717242160"},"user_tz":300},"id":"CS1SLW3sz-3o","outputId":"9c93ad51-b7f3-4e2b-8f83-e770c427a3c2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting ultralytics\n","  Downloading ultralytics-8.2.72-py3-none-any.whl.metadata (41 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.3.1+cu121)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.18.1+cu121)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.4)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.1.4)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n","Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n","  Downloading ultralytics_thop-2.0.0-py3-none-any.whl.metadata (8.5 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.53.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.7.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.15.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n","Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.3.1)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->ultralytics)\n","  Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n","Downloading ultralytics-8.2.72-py3-none-any.whl (863 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m863.7/863.7 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Downloading ultralytics_thop-2.0.0-py3-none-any.whl (25 kB)\n","Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 ultralytics-8.2.72 ultralytics-thop-2.0.0\n"]}],"source":["!pip install ultralytics"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":363,"status":"ok","timestamp":1722738194697,"user":{"displayName":"felix sarpong","userId":"04486120806717242160"},"user_tz":300},"id":"hD-UGrACMMZ-","outputId":"373aee4a-72db-4c4a-9b18-816b7cd7deb5"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","image 1/1 /content/10619969_366278796862949_668151654247090443_o.jpg: 640x480 6 persons, 1 skateboard, 11.0ms\n","Speed: 3.0ms preprocess, 11.0ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 480)\n"]}],"source":["# <------------------------ Section 1 --------------------------------->\n","from ultralytics import YOLO\n","\n","# Load a pretrained YOLOv8n model\n","model = YOLO('yolov8n.pt')\n","\n","# Run inference on an image\n","results = model('10619969_366278796862949_668151654247090443_o.jpg')  # results list\n","\n","# desired_class ids -> this is a list of class ids that you want to consider (the id should be same as it is given in coco)\n","desired_class_ids=[0,2,3]\n","desired_class_names=dict(({0,\"alpha\"},{2,\"gama\"},{3,\"delta\"})) # class labels corresponding to desired_class_ids\n","\n","# for checking the class_id corresponding to class_labels (for coco dataset) you can uncomment the below code\n","# print(results[0].names)\n","\n","# <--------------------------- Section 2 ------------------------------------->\n","# Store desired results only\n","for r in results:\n","    # indexes -> list of indexes of only that classes which you want consider\n","    indexes = [index for index, item in enumerate(r.boxes.cls) if item in desired_class_ids]\n","    # now we are storing results for classes that you want to consider\n","    class_ids=r.boxes.cls[indexes] # stores the class_detected (includes only the classes that you want to consider)\n","    class_confs=r.boxes.conf[indexes]\n","    boxes_xyxy=r.boxes.xyxy[indexes] # stores the bounding boxes in xyxy format\n","    boxes_xyxyn=r.boxes.xyxyn[indexes] # stores the bounding boxes in xyxy (normalized) format\n","    boxes_xywh=r.boxes.xywh[indexes] # stores the bounding boxes in xywh format\n"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":218},"executionInfo":{"elapsed":79819,"status":"ok","timestamp":1722862149809,"user":{"displayName":"felix sarpong","userId":"04486120806717242160"},"user_tz":300},"id":"1pIxs_2ANi_d","outputId":"3deecf68-0c46-455a-b4ed-ea8963717b0c"},"outputs":[{"name":"stdout","output_type":"stream","text":["  adding: content/results/ (stored 0%)\n","  adding: content/results/checkpoint-36985/ (stored 0%)\n","  adding: content/results/checkpoint-36985/config.json (deflated 51%)\n","  adding: content/results/checkpoint-36985/training_args.bin (deflated 52%)\n","  adding: content/results/checkpoint-36985/model.safetensors (deflated 7%)\n","  adding: content/results/checkpoint-36985/generation_config.json (deflated 24%)\n","  adding: content/results/checkpoint-36985/optimizer.pt (deflated 8%)\n","  adding: content/results/checkpoint-36985/scheduler.pt (deflated 56%)\n","  adding: content/results/checkpoint-36985/trainer_state.json (deflated 80%)\n","  adding: content/results/checkpoint-36985/rng_state.pth (deflated 25%)\n"]},{"data":{"application/javascript":"\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ","text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/javascript":"download(\"download_fb9c3b24-bd5c-4210-9cfe-bd6fa3e28397\", \"gpt.zip\", 1382733344)","text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"}],"source":["import locale\n","locale.getpreferredencoding = lambda: \"UTF-8\"\n","!zip -r /content/gpt.zip /content/results/\n","from google.colab import files\n","files.download(\"/content/gpt.zip\")\n"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1082,"status":"ok","timestamp":1722738276363,"user":{"displayName":"felix sarpong","userId":"04486120806717242160"},"user_tz":300},"id":"Jgz5wZ-XDUfs","outputId":"eff0d1ea-db51-4258-b9da-7f6219e9bc72"},"outputs":[{"data":{"text/plain":["array([[[234, 229, 228],\n","        [233, 228, 227],\n","        [232, 228, 227],\n","        ...,\n","        [237, 237, 237],\n","        [237, 237, 237],\n","        [237, 237, 237]],\n","\n","       [[231, 226, 225],\n","        [231, 226, 225],\n","        [229, 225, 224],\n","        ...,\n","        [237, 237, 237],\n","        [237, 237, 237],\n","        [237, 237, 237]],\n","\n","       [[227, 223, 222],\n","        [227, 223, 222],\n","        [226, 222, 221],\n","        ...,\n","        [237, 237, 237],\n","        [237, 237, 237],\n","        [237, 237, 237]],\n","\n","       ...,\n","\n","       [[255,  42,   4],\n","        [255,  42,   4],\n","        [255,  42,   4],\n","        ...,\n","        [156, 159, 150],\n","        [156, 159, 150],\n","        [156, 159, 150]],\n","\n","       [[255,  42,   4],\n","        [255,  42,   4],\n","        [255,  42,   4],\n","        ...,\n","        [156, 159, 150],\n","        [156, 159, 150],\n","        [156, 159, 150]],\n","\n","       [[255,  42,   4],\n","        [255,  42,   4],\n","        [255,  42,   4],\n","        ...,\n","        [155, 158, 149],\n","        [155, 158, 149],\n","        [155, 158, 149]]], dtype=uint8)"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["\n","\n","# <----------------------------- Section 3 ---------------------------------->\n","# code for visualizing the results\n","import cv2\n","import os\n","from ultralytics.utils.plotting import Annotator,colors\n","def visualize_detections(img_src, output_dir=\"runs/\", boxes_xyxy=None, class_ids=None, class_confs=None):\n","    \"\"\"\n","    This function visualizes the detections by plotting bounding boxes on the image.\n","\n","    Args:\n","    img_src: The input image path.\n","    output_dir (str, optional): The directory where the output will be saved. Defaults to \"runs/\".\n","    boxes_xyxy (list): The list of bounding boxes in the format (x1, y1, x2, y2).\n","    class_ids (list): The list of class IDs for each detected object.\n","    class_confs (list): The list of confidence scores for each detected object.\n","\n","    Returns:\n","    numpy array: The image with plotted bounding boxes.\n","    \"\"\"\n","    img=cv2.imread(img_src)\n","    # Create a copy of the image\n","    imgcpy = img.copy()\n","\n","    # Initialize the annotator\n","    annotator = Annotator(imgcpy)\n","\n","    # Loop over each detected object\n","    for i in range(len(class_ids)):\n","        # Add a bounding box and label to the image\n","        annotator.box_label(box=boxes_xyxy[i], label=(desired_class_names[class_ids[i].item()]+\" \"+ str(round(class_confs[0].item(), 2))), color=colors(class_ids[i], True))\n","\n","    # Check if output directory exists\n","    if not os.path.exists(output_dir):\n","        # If not, create it\n","        os.makedirs(output_dir)\n","\n","    # Save the image\n","    cv2.imwrite(os.path.join(output_dir, 'predicted_img.jpg'), annotator.result())\n","    # Return the annotated image\n","    return annotator.result()\n","\n","# run the above function to visualize the predited boxes\n","visualize_detections(\"10619969_366278796862949_668151654247090443_o.jpg\",boxes_xyxy=boxes_xyxy,class_ids=class_ids,class_confs=class_confs)\n","\n","# now your predicted image is saved at location runs/predicted_img.jpg"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10769964,"status":"ok","timestamp":1722736931091,"user":{"displayName":"felix sarpong","userId":"04486120806717242160"},"user_tz":300},"id":"V0nbUCPeDVXR","outputId":"898cfcc9-0bab-43da-e8f0-73bbbe7054b8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ultralytics YOLOv8.2.72 🚀 Python-3.10.12 torch-2.3.1+cu121 CUDA:0 (NVIDIA L4, 22700MiB)\n","\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=coco.yaml, epochs=5, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train2\n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n","  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n","  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n","  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n","  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n","  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n","  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n","  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n","  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n","  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n"," 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n"," 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n"," 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n"," 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n"," 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n"," 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n"," 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \n","Model summary: 225 layers, 3,157,200 parameters, 3,157,184 gradients, 8.9 GFLOPs\n","\n","Transferred 355/355 items from pretrained weights\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train2', view at http://localhost:6006/\n","Freezing layer 'model.22.dfl.conv.weight'\n","\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/coco/labels/train2017.cache... 117266 images, 1021 backgrounds, 0 corrupt: 100%|██████████| 118287/118287 [00:00<?, ?it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"]},{"name":"stderr","output_type":"stream","text":["/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/coco/labels/val2017.cache... 4952 images, 48 backgrounds, 0 corrupt: 100%|██████████| 5000/5000 [00:00<?, ?it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Plotting labels to runs/detect/train2/labels.jpg... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n","Image sizes 640 train, 640 val\n","Using 8 dataloader workers\n","Logging results to \u001b[1mruns/detect/train2\u001b[0m\n","Starting training for 5 epochs...\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["        1/5      4.28G      1.099       1.33       1.16        204        640: 100%|██████████| 7393/7393 [36:07<00:00,  3.41it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 157/157 [00:29<00:00,  5.28it/s]\n"]},{"name":"stdout","output_type":"stream","text":["                   all       5000      36335      0.614      0.457      0.497      0.349\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["        2/5      4.31G      1.116      1.365      1.171        111        640: 100%|██████████| 7393/7393 [35:31<00:00,  3.47it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 157/157 [00:28<00:00,  5.53it/s]\n"]},{"name":"stdout","output_type":"stream","text":["                   all       5000      36335      0.619      0.458        0.5      0.351\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["        3/5      3.86G      1.121      1.378      1.175        192        640: 100%|██████████| 7393/7393 [34:13<00:00,  3.60it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 157/157 [00:26<00:00,  6.02it/s]\n"]},{"name":"stdout","output_type":"stream","text":["                   all       5000      36335       0.63      0.459      0.503      0.356\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["        4/5      3.45G      1.111      1.358       1.17        185        640: 100%|██████████| 7393/7393 [33:55<00:00,  3.63it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 157/157 [00:25<00:00,  6.09it/s]\n"]},{"name":"stdout","output_type":"stream","text":["                   all       5000      36335      0.613      0.472      0.506      0.358\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["        5/5      3.38G      1.101      1.327      1.163        170        640: 100%|██████████| 7393/7393 [33:57<00:00,  3.63it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 157/157 [00:26<00:00,  6.02it/s]\n"]},{"name":"stdout","output_type":"stream","text":["                   all       5000      36335      0.619      0.468      0.509      0.361\n","\n","5 epochs completed in 2.938 hours.\n","Optimizer stripped from runs/detect/train2/weights/last.pt, 6.5MB\n","Optimizer stripped from runs/detect/train2/weights/best.pt, 6.5MB\n","\n","Validating runs/detect/train2/weights/best.pt...\n","Ultralytics YOLOv8.2.72 🚀 Python-3.10.12 torch-2.3.1+cu121 CUDA:0 (NVIDIA L4, 22700MiB)\n","Model summary (fused): 168 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n"]},{"name":"stderr","output_type":"stream","text":["                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 157/157 [00:44<00:00,  3.56it/s]\n"]},{"name":"stdout","output_type":"stream","text":["                   all       5000      36335      0.621      0.468      0.509      0.361\n","                person       2693      10777       0.77      0.656      0.738      0.508\n","               bicycle        149        314      0.632      0.379      0.439      0.252\n","                   car        535       1918      0.654      0.513       0.56       0.36\n","            motorcycle        159        367      0.684      0.559      0.645      0.404\n","              airplane         97        143       0.76      0.755      0.819      0.641\n","                   bus        189        283      0.737      0.653      0.735      0.616\n","                 train        157        190      0.808      0.774      0.834       0.64\n","                 truck        250        414      0.556      0.377      0.427      0.293\n","                  boat        121        424      0.551      0.337      0.381       0.21\n","         traffic light        191        634      0.636      0.334      0.394      0.202\n","          fire hydrant         86        101      0.909      0.693      0.761      0.615\n","             stop sign         69         75      0.685      0.627      0.693      0.632\n","         parking meter         37         60      0.554      0.483      0.544      0.433\n","                 bench        235        411      0.615      0.258      0.288      0.188\n","                  bird        125        427      0.649      0.375       0.43      0.278\n","                   cat        184        202      0.781      0.827      0.862      0.661\n","                   dog        177        218      0.662      0.693      0.707      0.572\n","                 horse        128        272      0.705      0.629      0.695      0.524\n","                 sheep         65        354       0.57      0.669      0.658      0.455\n","                   cow         87        372       0.73      0.573      0.657      0.457\n","              elephant         89        252      0.683      0.833      0.821      0.624\n","                  bear         49         71      0.796      0.817      0.842      0.681\n","                 zebra         85        266      0.809      0.808      0.877      0.658\n","               giraffe        101        232      0.846      0.823      0.878      0.673\n","              backpack        228        371      0.488       0.14      0.194      0.103\n","              umbrella        174        407      0.648      0.516      0.543      0.362\n","               handbag        292        540      0.419     0.0981       0.15     0.0771\n","                   tie        145        252      0.636      0.333      0.414      0.251\n","              suitcase        105        299       0.51      0.458      0.486       0.32\n","               frisbee         84        115      0.735      0.713      0.758      0.576\n","                  skis        120        241      0.602      0.301      0.333      0.174\n","             snowboard         49         69      0.494      0.319      0.363      0.249\n","           sports ball        169        260      0.741      0.441      0.467      0.314\n","                  kite         91        327      0.608       0.52      0.555      0.377\n","          baseball bat         97        145      0.536      0.331      0.368        0.2\n","        baseball glove        100        148      0.656      0.466      0.496      0.298\n","            skateboard        127        179      0.707      0.587      0.638      0.438\n","             surfboard        149        267       0.57      0.483      0.486      0.297\n","         tennis racket        167        225      0.719      0.591      0.661      0.393\n","                bottle        379       1013      0.593      0.394      0.447      0.292\n","            wine glass        110        341      0.656      0.357      0.415      0.266\n","                   cup        390        895      0.577      0.422       0.48      0.341\n","                  fork        155        215      0.635      0.307      0.384      0.259\n","                 knife        181        325      0.476       0.16      0.182      0.107\n","                 spoon        153        253       0.39      0.132      0.147     0.0859\n","                  bowl        314        623      0.598      0.486      0.527      0.387\n","                banana        103        370       0.51      0.318      0.349      0.214\n","                 apple         76        236      0.385      0.258      0.223      0.158\n","              sandwich         98        177      0.573      0.424      0.454      0.332\n","                orange         85        285      0.434      0.409      0.357      0.275\n","              broccoli         71        312       0.47      0.335       0.34      0.187\n","                carrot         81        365      0.455      0.274      0.298      0.182\n","               hot dog         51        125      0.593      0.416      0.466       0.34\n","                 pizza        153        284      0.665      0.629      0.659      0.496\n","                 donut         62        328      0.515       0.53      0.499      0.393\n","                  cake        124        310       0.56      0.413      0.425       0.28\n","                 chair        580       1771       0.58      0.331      0.389      0.247\n","                 couch        195        261      0.622      0.544      0.578      0.417\n","          potted plant        172        342      0.507      0.348      0.376      0.215\n","                   bed        149        163      0.533      0.525      0.578      0.435\n","          dining table        501        695      0.541      0.397      0.421      0.276\n","                toilet        149        179      0.714      0.704      0.749      0.617\n","                    tv        207        288      0.762      0.639      0.707      0.535\n","                laptop        183        231      0.657      0.649      0.697       0.57\n","                 mouse         88        106      0.628      0.651      0.696       0.52\n","                remote        145        283      0.442      0.201      0.275      0.159\n","              keyboard        106        153      0.597      0.588      0.635      0.471\n","            cell phone        214        262      0.559      0.351      0.386      0.268\n","             microwave         54         55      0.697      0.564      0.626      0.504\n","                  oven        115        143      0.581      0.446      0.503      0.336\n","               toaster          8          9      0.709      0.222      0.327      0.244\n","                  sink        187        225      0.594      0.453      0.486      0.311\n","          refrigerator        101        126      0.666      0.611      0.651      0.497\n","                  book        230       1129      0.458     0.0992      0.189     0.0909\n","                 clock        204        267      0.725      0.599      0.653      0.447\n","                  vase        137        274      0.507      0.456      0.435      0.303\n","              scissors         28         36      0.687      0.306      0.309      0.262\n","            teddy bear         94        190      0.605      0.553      0.589      0.403\n","            hair drier          9         11          1          0    0.00604    0.00455\n","            toothbrush         34         57      0.376      0.158      0.178      0.116\n","Speed: 0.1ms preprocess, 0.7ms inference, 0.0ms loss, 0.9ms postprocess per image\n","Saving runs/detect/train2/predictions.json...\n","\n","Evaluating pycocotools mAP using runs/detect/train2/predictions.json and /content/datasets/coco/annotations/instances_val2017.json...\n","loading annotations into memory...\n","Done (t=0.87s)\n","creating index...\n","index created!\n","Loading and preparing results...\n","DONE (t=7.30s)\n","creating index...\n","index created!\n","Running per image evaluation...\n","Evaluate annotation type *bbox*\n","DONE (t=73.40s).\n","Accumulating evaluation results...\n","DONE (t=18.08s).\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.364\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.514\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.396\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.178\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.402\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.509\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.317\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.531\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.587\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.363\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.658\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.766\n","Results saved to \u001b[1mruns/detect/train2\u001b[0m\n"]}],"source":["from ultralytics import YOLO\n","\n","# Load a model\n","model = YOLO(\"yolov8n.pt\")  # load a pretrained model (recommended for training)\n","\n","# Train the model\n","results = model.train(data=\"coco.yaml\", epochs=5, imgsz=640)"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":259,"referenced_widgets":["bb7b2f2cd48a4a6c8ca64326045d8802","4e7e6d97606549ffa483eefc73030694","9c9a9dfde7d84e7991c778369982fffd","1b3409e585b4459cb69a05f9d2a3b7c4","545b993d594e4d27a10a735c23daff2f","7ab28c79c97c43fdbeae60c0ece14bb1","9f95eb5701e44ddcad31ed1a10aa5aed","0ef240b4d58f447aaa7df7e6e30284a0","bdc825abb416432b957b6943cc09c76b","dea95e9051934367bf83d628ffdab922","e61f2cbcdca046b8965aa81902936409","eba381bad97c46b78a20587a1f74dac5","0ff0bf49bce24b3492a79ee90d3162a0","5050a8888d1d4f078433f7d1774c02bb","7633d33117144f1c976fb397ff8637ee","980fca79793c48a583b7cbcc6e7bef22","fe1a5b7e379e4e5b9f97cfbb02512d3c","e732f587fd9045f48728a1e6f639e66c","828c3be01ec24e92ab42fc5e09097460","7c537e01b6f344d1bf02fef9ab189170","0fa6844e269f46fbb352d6323d391bd2","de16e191a8ca4a3e9e3144b4a7c4cd68","483dec2047664fe687974ed7a57e3192","a872d9fd58514b4f814f007c587d44f2","23d072b226ab460f849d5e6eaac9d7ce","6205343029bc4bf783a5a9def245a93d","0e59270d1a0a4bc6a75f1437156c4d76","091b6db18cae47439e33d8178fc35411","b68d0d427475467da7ce70b72484c33b","a962522123764b1f9a88356abc3a277e","a82d2a41740a4c6eb1f92f509abc066d","383c7e1e749548d8b0b0e5cb96fe34bb","faf27d9f8b2b4c5e9a327fef6661e437","fa39d6213fa0416481588df69745aa78","87c3d18bafd5413eaa0d15d462adf58f","7986714b009c4022862d64163ec61056","b1cb4f565d4b4e21a4866b80810ce73d","957840ccf04b4372a3d06bf4e10d00a4","8909c75759484e778958acf0dce89092","f9d05e755b514b458d94d61c3022ee3a","48821c24848141faa5a96cc9638cdd9b","205c25bc8f0e4aebae901b06d8daa713","7fd26a4b40a6495c97ba5db0efba44e2","0624c4c7d3794c4ca5ccaa441087c242","7c1743cda78d49009d4ba023decd469e","304b0d2fb66f4fd19af215f0fc559cf7","a655f2cd73c140c9946ddcff19403605","b876c1e4074b486eb4cf9ede15f103d9","ebb0fd7ff1fe4839bc8d73c9e43b0335","baae0f57a45c4e55b9ee6d3e53585bc0","3deafeca487447799e58a2578a3689ff","66db2f7e1b6a4b159ca5ee4ef1015ba8","8b5d5ad6318244eb9db358a6ae24d453","cb234be00bdb4bc2a0b2ca8889dfe897","1c910be9dcf04291957d421c7caceb6b","83d73683204b48af8fb1da92afbc6960","7d7b62c9679f4571a8b619c3fcdc5f26","f77c7cd46e8144d4a4e255ec872f5ce0","1c729dac02914d148d7a6f3249bed406","b7b2eaf12be04e679bad5eee2c199571","4a3e03609d2e4dfba0dab5530c65c711","610c2bb20a80463793defd59ecb552a9","4c7d477c20ae4d8199276eae9ef25eaa","d6bb1a764d2e4cf481616f0f61a68ffb","f19e395c23e64abc88e0d4f578c626e8","f0eb9f48aca24200938b075eecbc554d","40f2263135524a80b0f1f309fbae3459","308f2f6cc72040ad8512a629bc4f60dd","f680f552e8664117b49f94fab6a9de7b","f6078e16c5744014b94220f1e789bf65","d6b9bd1beb5e486487bbd87c0d20ab1d","cdd7c05a0970457a8a535390385049e8","9bfb5b54e4a547a7867a7ddad2c8641e","cc997165e0de4d198f5d5739a6513d2a","e7ac92133853429c932a892dbb918735","4bfb63d9cbe94c9996d2aa501a53bb81","993e8f3d583e4b62ae9b2468813f8db7"]},"executionInfo":{"elapsed":13641,"status":"ok","timestamp":1722842351079,"user":{"displayName":"felix sarpong","userId":"04486120806717242160"},"user_tz":300},"id":"vzSRPtmNwfmv","outputId":"8ac3bdc5-2f15-4757-cad9-551e4d8ee990"},"outputs":[],"source":["from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n","import torch\n","from torch.utils.data import Dataset\n","import json\n","\n","\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2',\n","                                          bos_token='<|startoftext|>',\n","                                          eos_token='<|endoftext|>',\n","                                          pad_token='<|pad|>')"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The max model length is 1024 for this model\n","The beginning of sequence token <|startoftext|> token has the id 50257\n","The end of sequence token <|endoftext|> has the id 50256\n","The padding token <|pad|> has the id 50258\n"]}],"source":["print(\"The max model length is {} for this model\".format(tokenizer.model_max_length))\n","print(\"The beginning of sequence token {} token has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.bos_token_id), tokenizer.bos_token_id))\n","print(\"The end of sequence token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.eos_token_id), tokenizer.eos_token_id))\n","print(\"The padding token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.pad_token_id), tokenizer.pad_token_id))\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1722842351079,"user":{"displayName":"felix sarpong","userId":"04486120806717242160"},"user_tz":300},"id":"MgGBUEn4wgrc"},"outputs":[],"source":["import json\n","import random\n","# Load your full dataset\n","with open('custom_dataset.json', 'r') as file:\n","    dataset = json.load(file)\n","\n","# Randomly sample a subset of the dataset\n","subset_size = int(len(dataset) * 0.02)  # Adjust the fraction as needed\n","subset = random.sample(dataset, subset_size)\n","\n","# Save the subset\n","with open('subset_dataset.json', 'w') as file:\n","    json.dump(subset, file, indent=4)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":46,"metadata":{"executionInfo":{"elapsed":176,"status":"ok","timestamp":1722842351253,"user":{"displayName":"felix sarpong","userId":"04486120806717242160"},"user_tz":300},"id":"oIJSe8dzwk-y"},"outputs":[],"source":["# Load the dataset\n","with open('subset_dataset.json', 'r') as f:\n","    train_data = json.load(f)\n","\n","with open('custom_eval_dataset.json', 'r') as f:\n","    val_data = json.load(f)\n","\n","# Function to format the objects and coordinates\n","def format_objects(objects):\n","    formatted_objects = []\n","    for obj in objects:\n","        label = obj['label']\n","        coordinates = ', '.join(map(str, obj['coordinates']))\n","        formatted_objects.append(f\"{label} with coordinates [{coordinates}]\")\n","    return 'The objects detected in the image are: ' + ', '.join(formatted_objects) + '. Describe the scene.'\n"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[],"source":["# Prepare the training data\n","train_texts = [format_objects(item['objects']) for item in train_data]\n","train_descriptions = [item['description'] for item in train_data]\n","\n","# Prepare the validation data\n","val_texts = [format_objects(item['objects']) for item in val_data]\n","val_descriptions = [item['description'] for item in val_data]\n"]},{"cell_type":"code","execution_count":59,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":642672,"status":"ok","timestamp":1722843602434,"user":{"displayName":"felix sarpong","userId":"04486120806717242160"},"user_tz":300},"id":"W8-bAC9hwnKD","outputId":"8aa74bf7-4e84-47b3-fbcc-3aa21b810805"},"outputs":[],"source":["max_length = 512\n","#  Tokenize inputs and labels\n","train_inputs = tokenizer(train_texts, return_tensors='pt', max_length=max_length, truncation=True, padding='max_length')\n","train_labels = tokenizer(train_descriptions, return_tensors='pt', max_length=max_length, truncation=True, padding='max_length')\n","\n","# Prepare TensorDataset and DataLoader\n","train_dataset = TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels['input_ids'])\n","train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n","\n","\n","val_inputs = tokenizer(val_texts, return_tensors='pt', max_length=max_length, truncation=True, padding='max_length')\n","val_labels = tokenizer(val_descriptions, return_tensors='pt', max_length=max_length, truncation=True, padding='max_length')\n","\n","val_dataset = TensorDataset(val_inputs['input_ids'], val_inputs['attention_mask'], val_labels['input_ids'])\n","val_dataloader = DataLoader(val_dataset, batch_size=4)\n","\n","# Create DataLoaders\n"]},{"cell_type":"code","execution_count":63,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1722843602435,"user":{"displayName":"felix sarpong","userId":"04486120806717242160"},"user_tz":300},"id":"yLQSR7JKBXpx"},"outputs":[{"ename":"KeyError","evalue":"'labels'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[63], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, TensorDataset\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Create DataLoaders\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m TensorDataset(train_inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m], train_inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m], train_inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      4\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m TensorDataset(val_inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m], val_inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m], val_inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m])\n","File \u001b[0;32m~/anaconda3/envs/DeepLearing/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:253\u001b[0m, in \u001b[0;36mBatchEncoding.__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;124;03mIf the key is a string, returns the value of the dict associated to `key` ('input_ids', 'attention_mask',\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;124;03metc.).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;124;03mwith the constraint of slice.\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[item]\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encodings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encodings[item]\n","\u001b[0;31mKeyError\u001b[0m: 'labels'"]}],"source":["from torch.utils.data import DataLoader, TensorDataset\n","# Create DataLoaders\n","train_dataset = TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_inputs['labels'])\n","train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n","\n","val_dataset = TensorDataset(val_inputs['input_ids'], val_inputs['attention_mask'], val_inputs['labels'])\n","val_dataloader = DataLoader(val_dataset, batch_size=4)"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["import numpy as np\n","import random\n","\n","from transformers import GPT2Tokenizer,GPT2Config, GPT2LMHeadModel, Trainer, TrainingArguments\n","# Loading the model configuration and setting it to the GPT2 standard settings.\n","configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n","\n","# Create the instance of the model and set the token size embedding length\n","model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=configuration)\n","model.resize_token_embeddings(len(tokenizer))\n","# Tell pytorch to run this model on the GPU.\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","# This step is optional but will enable reproducible runs.\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[],"source":["\n","\n","# We wil create a few variables to define the training parameters of the model\n","# epochs are the training rounds\n","# the warmup steps are steps at the start of training that are ignored\n","# every x steps we will sample the model to test the output\n","\n","epochs = 10\n","warmup_steps = 1e2\n","sample_every = 100\n","\n"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[],"source":["\n","\n","from transformers import AdamW\n","# AdamW is a class from the huggingface library, it is the optimizer we will be using, and\n","# we will only be instantiating it with the default parameters.\n","\n","optimizer = AdamW(model.parameters(),\n","                  lr = 5e-4,\n","                  eps = 1e-8\n","                )\n","\n"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[],"source":["from transformers import get_linear_schedule_with_warmup\n","\n","\"\"\"\n","Total training steps is the number of data points, times the number of epochs.\n","Essentially, epochs are training cycles, how many times each point will be seen by the model.\n","\"\"\"\n","\n","total_steps = len(train_dataloader) * epochs\n","\n","\"\"\"\n","We can set a variable learning rate which will help scan larger areas of the\n","problem space at higher LR earlier, then fine tune to find the exact model minima\n","at lower LR later in training.\n","\"\"\"\n","scheduler = get_linear_schedule_with_warmup(optimizer,\n","                                            num_warmup_steps = warmup_steps,\n","                                            num_training_steps = total_steps)"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Beginning epoch 1 of 10\n","Step 0: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 1: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 2: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 3: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 4: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 5: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 6: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 7: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 8: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 9: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 10: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 11: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 12: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 13: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 14: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 15: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 16: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 17: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 18: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 19: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 20: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 21: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 22: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 23: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 24: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 25: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 26: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 27: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 28: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 29: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 30: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 31: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 32: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 33: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 34: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 35: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 36: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 37: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 38: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 39: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 40: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 41: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 42: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 43: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 44: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 45: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 46: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 47: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 48: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 49: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 50: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 51: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 52: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 53: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 54: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 55: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 56: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 57: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 58: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 59: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 60: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 61: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 62: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 63: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 64: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 65: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 66: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 67: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 68: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 69: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 70: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 71: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 72: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 73: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 74: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 75: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 76: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 77: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 78: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 79: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 80: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 81: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 82: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 83: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 84: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 85: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 86: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 87: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 88: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 89: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 90: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 91: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 92: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 93: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 94: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 95: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 96: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 97: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 98: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 99: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Step 100: Input IDs shape: torch.Size([4, 512]), Labels shape: torch.Size([4, 512])\n","Batch 100 of 2959. Loss: 0.19254548847675323. Time: 0:07:49\n"]},{"ename":"RuntimeError","evalue":"The size of tensor a (2048) must match the size of tensor b (4) at non-singleton dimension 0","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[62], line 55\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_dataloader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00melapsed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     54\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 55\u001b[0m sample_outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m     56\u001b[0m     bos_token_id\u001b[38;5;241m=\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m1\u001b[39m, tokenizer\u001b[38;5;241m.\u001b[39mvocab_size),\n\u001b[1;32m     57\u001b[0m     do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     58\u001b[0m     top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m     59\u001b[0m     max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,\n\u001b[1;32m     60\u001b[0m     top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m,\n\u001b[1;32m     61\u001b[0m     num_return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     62\u001b[0m     pad_token_id\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad_token_id,  \u001b[38;5;66;03m# Set pad_token_id\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mb_masks  \u001b[38;5;66;03m# Pass attention_mask\u001b[39;00m\n\u001b[1;32m     64\u001b[0m )\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, sample_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sample_outputs):\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExample output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer\u001b[38;5;241m.\u001b[39mdecode(sample_output,\u001b[38;5;250m \u001b[39mskip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[0;32m~/anaconda3/envs/DeepLearing/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m~/anaconda3/envs/DeepLearing/lib/python3.11/site-packages/transformers/generation/utils.py:1525\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1518\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1519\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1520\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1521\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1522\u001b[0m     )\n\u001b[1;32m   1524\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[1;32m   1526\u001b[0m         input_ids,\n\u001b[1;32m   1527\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[1;32m   1528\u001b[0m         logits_warper\u001b[38;5;241m=\u001b[39mlogits_warper,\n\u001b[1;32m   1529\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[1;32m   1530\u001b[0m         pad_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mpad_token_id,\n\u001b[1;32m   1531\u001b[0m         eos_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39meos_token_id,\n\u001b[1;32m   1532\u001b[0m         output_scores\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39moutput_scores,\n\u001b[1;32m   1533\u001b[0m         return_dict_in_generate\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mreturn_dict_in_generate,\n\u001b[1;32m   1534\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[1;32m   1535\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[1;32m   1536\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1537\u001b[0m     )\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1540\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1541\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1542\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1543\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1548\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1549\u001b[0m     )\n","File \u001b[0;32m~/anaconda3/envs/DeepLearing/lib/python3.11/site-packages/transformers/generation/utils.py:2664\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2662\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pad_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2663\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf `eos_token_id` is defined, make sure that `pad_token_id` is defined.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2664\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m next_tokens \u001b[38;5;241m*\u001b[39m unfinished_sequences \u001b[38;5;241m+\u001b[39m pad_token_id \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m unfinished_sequences)\n\u001b[1;32m   2666\u001b[0m \u001b[38;5;66;03m# update generated ids, model inputs, and length for next step\u001b[39;00m\n\u001b[1;32m   2667\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([input_ids, next_tokens[:, \u001b[38;5;28;01mNone\u001b[39;00m]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2048) must match the size of tensor b (4) at non-singleton dimension 0"]}],"source":["import random\n","import time\n","import datetime\n","import torch\n","\n","def format_time(elapsed):\n","    return str(datetime.timedelta(seconds=int(round((elapsed)))))\n","\n","# Set total training start time\n","total_t0 = time.time()\n","\n","# Initialize the list to track training statistics\n","training_stats = []\n","\n","# Move model to the device\n","model = model.to(device)\n","\n","for epoch_i in range(epochs):\n","    print(f'Beginning epoch {epoch_i + 1} of {epochs}')\n","    \n","    # Set epoch start time\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch\n","    total_train_loss = 0\n","\n","    # Put the model into training mode\n","    model.train()\n","\n","    for step, batch in enumerate(train_dataloader):\n","        b_input_ids = batch[0].to(device)\n","        b_masks = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","        \n","        # Debugging: Print shapes of input and labels to ensure they match\n","        print(f\"Step {step}: Input IDs shape: {b_input_ids.shape}, Labels shape: {b_labels.shape}\")\n","\n","        # Clear previously calculated gradients\n","        model.zero_grad()\n","\n","        # Perform a forward pass\n","        outputs = model(input_ids=b_input_ids, attention_mask=b_masks, labels=b_labels)\n","        loss = outputs.loss\n","\n","        # Accumulate the training loss\n","        batch_loss = loss.item()\n","        total_train_loss += batch_loss\n","\n","        # Print sample output every 100 batches\n","        if step % 100 == 0 and not step == 0:\n","            elapsed = format_time(time.time() - t0)\n","            print(f'Batch {step} of {len(train_dataloader)}. Loss: {batch_loss}. Time: {elapsed}')\n","\n","            model.eval()\n","            sample_outputs = model.generate(\n","                bos_token_id=random.randint(1, tokenizer.vocab_size),\n","                do_sample=True,\n","                top_k=50,\n","                max_length=200,\n","                top_p=0.95,\n","                num_return_sequences=1\n","            )\n","            for i, sample_output in enumerate(sample_outputs):\n","                print(f'Example output: {tokenizer.decode(sample_output, skip_special_tokens=True)}')\n","\n","            model.train()\n","\n","        # Perform a backward pass to calculate the gradients\n","        loss.backward()\n","\n","        # Update parameters and learning rate\n","        optimizer.step()\n","        scheduler.step()\n","\n","    # Calculate the average loss over all batches\n","    avg_train_loss = total_train_loss / len(train_dataloader)\n","\n","    # Measure how long this epoch took\n","    training_time = format_time(time.time() - t0)\n","    print(f'Average Training Loss: {avg_train_loss}. Epoch time: {training_time}')\n","\n","    # ==========================\n","    #       Validation\n","    # ==========================\n","\n","    # Set the model to evaluation mode\n","    model.eval()\n","\n","    total_eval_loss = 0\n","\n","    # Evaluate data for one epoch\n","    for batch in val_dataloader:\n","        b_input_ids = batch[0].to(device)\n","        b_masks = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        with torch.no_grad():\n","            outputs = model(input_ids=b_input_ids, attention_mask=b_masks, labels=b_labels)\n","            loss = outputs.loss\n","\n","        # Accumulate the validation loss\n","        batch_loss = loss.item()\n","        total_eval_loss += batch_loss\n","\n","    # Calculate the average loss over all validation batches\n","    avg_val_loss = total_eval_loss / len(val_dataloader)\n","\n","    # Measure how long the validation took\n","    validation_time = format_time(time.time() - t0)\n","    print(f'Validation Loss: {avg_val_loss}. Validation Time: {validation_time}')\n","\n","    # Record all statistics from this epoch\n","    training_stats.append({\n","        'epoch': epoch_i + 1,\n","        'Training Loss': avg_train_loss,\n","        'Validation Loss': avg_val_loss,\n","        'Training Time': training_time,\n","        'Validation Time': validation_time\n","    })\n","\n","print(f'Total training took {format_time(time.time() - total_t0)}')\n","\n","# Save the trained model\n","model.save_pretrained('fine_tuned_gpt2')\n","tokenizer.save_pretrained('fine_tuned_gpt2')\n","\n","# Save training statistics\n","import pandas as pd\n","df_stats = pd.DataFrame(training_stats)\n","df_stats.to_csv('training_stats.csv', index=False)\n"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29723,"status":"ok","timestamp":1722863921058,"user":{"displayName":"felix sarpong","userId":"04486120806717242160"},"user_tz":300},"id":"Tzn6H-1Pi-u7","outputId":"cc0463be-cb97-4f46-cf62-cd610e3056d0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Generate a description of a scene with: wine glass at coordinates [139.67, 279.84, 77.97, 91.24], wine glass at coordinates [83.87, 336.24, 123.04, 87.76], cake at coordinates [285.53, 208.66, 354.47, 210.09], dining table at coordinates [0.0, 392.83, 252.2, 25.33]. more \" it  also which would far or most than rather approach man purchasing and ref single ( one trans tried the first second class macro mis, other type thinking to person very was third early de whole character young is\n"]}],"source":["def generate_description(objects):\n","    input_text = format_objects(objects)\n","    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n","    \n","    with torch.no_grad():\n","        output = model.generate(input_ids, max_length=150, num_return_sequences=1, no_repeat_ngram_size=2, early_stopping=True)\n","    \n","    description = tokenizer.decode(output[0], skip_special_tokens=True)\n","    return description\n","\n","# Example usage\n","new_objects = [\n","    {\n","        \"label\": \"person\",\n","        \"coordinates\": [227.74, 187.5, 77.16, 171.12]\n","    },\n","    {\n","        \"label\": \"kite\",\n","        \"coordinates\": [399.06, 127.59, 58.23, 32.3]\n","    }\n","]\n","\n","print(generate_description(new_objects))\n"]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/Apple/anaconda3/envs/DeepLearing/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","/Users/Apple/anaconda3/envs/DeepLearing/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:433: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"ename":"TypeError","evalue":"sequence item 54: expected str instance, NoneType found","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[65], line 38\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     33\u001b[0m new_objects \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     34\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperson\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoordinates\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m227.74\u001b[39m, \u001b[38;5;241m187.5\u001b[39m, \u001b[38;5;241m77.16\u001b[39m, \u001b[38;5;241m171.12\u001b[39m]},\n\u001b[1;32m     35\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkite\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoordinates\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m399.06\u001b[39m, \u001b[38;5;241m127.59\u001b[39m, \u001b[38;5;241m58.23\u001b[39m, \u001b[38;5;241m32.3\u001b[39m]}\n\u001b[1;32m     36\u001b[0m ]\n\u001b[0;32m---> 38\u001b[0m description \u001b[38;5;241m=\u001b[39m generate_description(model, tokenizer, new_objects)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(description)\n","Cell \u001b[0;32mIn[65], line 29\u001b[0m, in \u001b[0;36mgenerate_description\u001b[0;34m(model, tokenizer, objects)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     27\u001b[0m     output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(input_ids, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m, num_return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, no_repeat_ngram_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, early_stopping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 29\u001b[0m description \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m description\n","File \u001b[0;32m~/anaconda3/envs/DeepLearing/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3756\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3753\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3754\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3756\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode(\n\u001b[1;32m   3757\u001b[0m     token_ids\u001b[38;5;241m=\u001b[39mtoken_ids,\n\u001b[1;32m   3758\u001b[0m     skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[1;32m   3759\u001b[0m     clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3761\u001b[0m )\n","File \u001b[0;32m~/anaconda3/envs/DeepLearing/lib/python3.11/site-packages/transformers/tokenization_utils.py:1024\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   1022\u001b[0m         current_sub_text\u001b[38;5;241m.\u001b[39mappend(token)\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_sub_text:\n\u001b[0;32m-> 1024\u001b[0m     sub_texts\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_string(current_sub_text))\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spaces_between_special_tokens:\n\u001b[1;32m   1027\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(sub_texts)\n","File \u001b[0;32m~/anaconda3/envs/DeepLearing/lib/python3.11/site-packages/transformers/models/gpt2/tokenization_gpt2.py:322\u001b[0m, in \u001b[0;36mGPT2Tokenizer.convert_tokens_to_string\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_tokens_to_string\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens):\n\u001b[1;32m    321\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(tokens)\n\u001b[1;32m    323\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbyte_decoder[c] \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m text])\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors)\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n","\u001b[0;31mTypeError\u001b[0m: sequence item 54: expected str instance, NoneType found"]}],"source":["from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","import torch\n","\n","# Load the fine-tuned model\n","model_path = 'fine_tuned_gpt2'\n","model = GPT2LMHeadModel.from_pretrained(model_path)\n","tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n","\n","# Move model to GPU if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","model.eval()\n","\n","def format_objects(objects):\n","    formatted_objects = []\n","    for obj in objects:\n","        label = obj['label']\n","        coordinates = ', '.join(map(str, obj['coordinates']))\n","        formatted_objects.append(f\"{label} with coordinates [{coordinates}]\")\n","    return 'The objects detected in the image are: ' + ', '.join(formatted_objects) + '. Describe the scene.'\n","\n","def generate_description(model, tokenizer, objects):\n","    input_text = format_objects(objects)\n","    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n","    \n","    with torch.no_grad():\n","        output = model.generate(input_ids, max_length=150, num_return_sequences=1, no_repeat_ngram_size=2, early_stopping=True)\n","    \n","    description = tokenizer.decode(output[0], skip_special_tokens=True)\n","    return description\n","\n","# Example usage\n","new_objects = [\n","    {\"label\": \"person\", \"coordinates\": [227.74, 187.5, 77.16, 171.12]},\n","    {\"label\": \"kite\", \"coordinates\": [399.06, 127.59, 58.23, 32.3]}\n","]\n","\n","description = generate_description(model, tokenizer, new_objects)\n","print(description)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":192,"status":"ok","timestamp":1722842612581,"user":{"displayName":"felix sarpong","userId":"04486120806717242160"},"user_tz":300},"id":"WTPhLE0Aa_P5","outputId":"89d91ec4-7776-4abe-effc-047a68a39114"},"outputs":[{"name":"stdout","output_type":"stream","text":["42481811456\n"]}],"source":["import torch\n","print(torch.cuda.get_device_properties(0).total_memory)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNhCGHKUkaZ6RpuTSm74uaF","gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0624c4c7d3794c4ca5ccaa441087c242":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"091b6db18cae47439e33d8178fc35411":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e59270d1a0a4bc6a75f1437156c4d76":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ef240b4d58f447aaa7df7e6e30284a0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0fa6844e269f46fbb352d6323d391bd2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ff0bf49bce24b3492a79ee90d3162a0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fe1a5b7e379e4e5b9f97cfbb02512d3c","placeholder":"​","style":"IPY_MODEL_e732f587fd9045f48728a1e6f639e66c","value":"model.safetensors: 100%"}},"1b3409e585b4459cb69a05f9d2a3b7c4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dea95e9051934367bf83d628ffdab922","placeholder":"​","style":"IPY_MODEL_e61f2cbcdca046b8965aa81902936409","value":" 665/665 [00:00&lt;00:00, 51.8kB/s]"}},"1c729dac02914d148d7a6f3249bed406":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f19e395c23e64abc88e0d4f578c626e8","placeholder":"​","style":"IPY_MODEL_f0eb9f48aca24200938b075eecbc554d","value":" 456k/456k [00:00&lt;00:00, 3.52MB/s]"}},"1c910be9dcf04291957d421c7caceb6b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"205c25bc8f0e4aebae901b06d8daa713":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"23d072b226ab460f849d5e6eaac9d7ce":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a962522123764b1f9a88356abc3a277e","max":124,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a82d2a41740a4c6eb1f92f509abc066d","value":124}},"304b0d2fb66f4fd19af215f0fc559cf7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_baae0f57a45c4e55b9ee6d3e53585bc0","placeholder":"​","style":"IPY_MODEL_3deafeca487447799e58a2578a3689ff","value":"vocab.json: 100%"}},"308f2f6cc72040ad8512a629bc4f60dd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cdd7c05a0970457a8a535390385049e8","placeholder":"​","style":"IPY_MODEL_9bfb5b54e4a547a7867a7ddad2c8641e","value":"tokenizer.json: 100%"}},"383c7e1e749548d8b0b0e5cb96fe34bb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3deafeca487447799e58a2578a3689ff":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"40f2263135524a80b0f1f309fbae3459":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_308f2f6cc72040ad8512a629bc4f60dd","IPY_MODEL_f680f552e8664117b49f94fab6a9de7b","IPY_MODEL_f6078e16c5744014b94220f1e789bf65"],"layout":"IPY_MODEL_d6b9bd1beb5e486487bbd87c0d20ab1d"}},"483dec2047664fe687974ed7a57e3192":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a872d9fd58514b4f814f007c587d44f2","IPY_MODEL_23d072b226ab460f849d5e6eaac9d7ce","IPY_MODEL_6205343029bc4bf783a5a9def245a93d"],"layout":"IPY_MODEL_0e59270d1a0a4bc6a75f1437156c4d76"}},"48821c24848141faa5a96cc9638cdd9b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a3e03609d2e4dfba0dab5530c65c711":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4bfb63d9cbe94c9996d2aa501a53bb81":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c7d477c20ae4d8199276eae9ef25eaa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e7e6d97606549ffa483eefc73030694":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7ab28c79c97c43fdbeae60c0ece14bb1","placeholder":"​","style":"IPY_MODEL_9f95eb5701e44ddcad31ed1a10aa5aed","value":"config.json: 100%"}},"5050a8888d1d4f078433f7d1774c02bb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_828c3be01ec24e92ab42fc5e09097460","max":548105171,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7c537e01b6f344d1bf02fef9ab189170","value":548105171}},"545b993d594e4d27a10a735c23daff2f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"610c2bb20a80463793defd59ecb552a9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6205343029bc4bf783a5a9def245a93d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_383c7e1e749548d8b0b0e5cb96fe34bb","placeholder":"​","style":"IPY_MODEL_faf27d9f8b2b4c5e9a327fef6661e437","value":" 124/124 [00:00&lt;00:00, 11.5kB/s]"}},"66db2f7e1b6a4b159ca5ee4ef1015ba8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7633d33117144f1c976fb397ff8637ee":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0fa6844e269f46fbb352d6323d391bd2","placeholder":"​","style":"IPY_MODEL_de16e191a8ca4a3e9e3144b4a7c4cd68","value":" 548M/548M [00:01&lt;00:00, 344MB/s]"}},"7986714b009c4022862d64163ec61056":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_48821c24848141faa5a96cc9638cdd9b","max":26,"min":0,"orientation":"horizontal","style":"IPY_MODEL_205c25bc8f0e4aebae901b06d8daa713","value":26}},"7ab28c79c97c43fdbeae60c0ece14bb1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c1743cda78d49009d4ba023decd469e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_304b0d2fb66f4fd19af215f0fc559cf7","IPY_MODEL_a655f2cd73c140c9946ddcff19403605","IPY_MODEL_b876c1e4074b486eb4cf9ede15f103d9"],"layout":"IPY_MODEL_ebb0fd7ff1fe4839bc8d73c9e43b0335"}},"7c537e01b6f344d1bf02fef9ab189170":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7d7b62c9679f4571a8b619c3fcdc5f26":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a3e03609d2e4dfba0dab5530c65c711","placeholder":"​","style":"IPY_MODEL_610c2bb20a80463793defd59ecb552a9","value":"merges.txt: 100%"}},"7fd26a4b40a6495c97ba5db0efba44e2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"828c3be01ec24e92ab42fc5e09097460":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"83d73683204b48af8fb1da92afbc6960":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7d7b62c9679f4571a8b619c3fcdc5f26","IPY_MODEL_f77c7cd46e8144d4a4e255ec872f5ce0","IPY_MODEL_1c729dac02914d148d7a6f3249bed406"],"layout":"IPY_MODEL_b7b2eaf12be04e679bad5eee2c199571"}},"87c3d18bafd5413eaa0d15d462adf58f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8909c75759484e778958acf0dce89092","placeholder":"​","style":"IPY_MODEL_f9d05e755b514b458d94d61c3022ee3a","value":"tokenizer_config.json: 100%"}},"8909c75759484e778958acf0dce89092":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b5d5ad6318244eb9db358a6ae24d453":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"957840ccf04b4372a3d06bf4e10d00a4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"980fca79793c48a583b7cbcc6e7bef22":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"993e8f3d583e4b62ae9b2468813f8db7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9bfb5b54e4a547a7867a7ddad2c8641e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9c9a9dfde7d84e7991c778369982fffd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0ef240b4d58f447aaa7df7e6e30284a0","max":665,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bdc825abb416432b957b6943cc09c76b","value":665}},"9f95eb5701e44ddcad31ed1a10aa5aed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a655f2cd73c140c9946ddcff19403605":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_66db2f7e1b6a4b159ca5ee4ef1015ba8","max":1042301,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8b5d5ad6318244eb9db358a6ae24d453","value":1042301}},"a82d2a41740a4c6eb1f92f509abc066d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a872d9fd58514b4f814f007c587d44f2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_091b6db18cae47439e33d8178fc35411","placeholder":"​","style":"IPY_MODEL_b68d0d427475467da7ce70b72484c33b","value":"generation_config.json: 100%"}},"a962522123764b1f9a88356abc3a277e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1cb4f565d4b4e21a4866b80810ce73d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7fd26a4b40a6495c97ba5db0efba44e2","placeholder":"​","style":"IPY_MODEL_0624c4c7d3794c4ca5ccaa441087c242","value":" 26.0/26.0 [00:00&lt;00:00, 2.16kB/s]"}},"b68d0d427475467da7ce70b72484c33b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b7b2eaf12be04e679bad5eee2c199571":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b876c1e4074b486eb4cf9ede15f103d9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cb234be00bdb4bc2a0b2ca8889dfe897","placeholder":"​","style":"IPY_MODEL_1c910be9dcf04291957d421c7caceb6b","value":" 1.04M/1.04M [00:00&lt;00:00, 7.75MB/s]"}},"baae0f57a45c4e55b9ee6d3e53585bc0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bb7b2f2cd48a4a6c8ca64326045d8802":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4e7e6d97606549ffa483eefc73030694","IPY_MODEL_9c9a9dfde7d84e7991c778369982fffd","IPY_MODEL_1b3409e585b4459cb69a05f9d2a3b7c4"],"layout":"IPY_MODEL_545b993d594e4d27a10a735c23daff2f"}},"bdc825abb416432b957b6943cc09c76b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cb234be00bdb4bc2a0b2ca8889dfe897":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc997165e0de4d198f5d5739a6513d2a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cdd7c05a0970457a8a535390385049e8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6b9bd1beb5e486487bbd87c0d20ab1d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6bb1a764d2e4cf481616f0f61a68ffb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"de16e191a8ca4a3e9e3144b4a7c4cd68":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dea95e9051934367bf83d628ffdab922":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e61f2cbcdca046b8965aa81902936409":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e732f587fd9045f48728a1e6f639e66c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e7ac92133853429c932a892dbb918735":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"eba381bad97c46b78a20587a1f74dac5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0ff0bf49bce24b3492a79ee90d3162a0","IPY_MODEL_5050a8888d1d4f078433f7d1774c02bb","IPY_MODEL_7633d33117144f1c976fb397ff8637ee"],"layout":"IPY_MODEL_980fca79793c48a583b7cbcc6e7bef22"}},"ebb0fd7ff1fe4839bc8d73c9e43b0335":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f0eb9f48aca24200938b075eecbc554d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f19e395c23e64abc88e0d4f578c626e8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6078e16c5744014b94220f1e789bf65":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4bfb63d9cbe94c9996d2aa501a53bb81","placeholder":"​","style":"IPY_MODEL_993e8f3d583e4b62ae9b2468813f8db7","value":" 1.36M/1.36M [00:00&lt;00:00, 6.85MB/s]"}},"f680f552e8664117b49f94fab6a9de7b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc997165e0de4d198f5d5739a6513d2a","max":1355256,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e7ac92133853429c932a892dbb918735","value":1355256}},"f77c7cd46e8144d4a4e255ec872f5ce0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4c7d477c20ae4d8199276eae9ef25eaa","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d6bb1a764d2e4cf481616f0f61a68ffb","value":456318}},"f9d05e755b514b458d94d61c3022ee3a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fa39d6213fa0416481588df69745aa78":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_87c3d18bafd5413eaa0d15d462adf58f","IPY_MODEL_7986714b009c4022862d64163ec61056","IPY_MODEL_b1cb4f565d4b4e21a4866b80810ce73d"],"layout":"IPY_MODEL_957840ccf04b4372a3d06bf4e10d00a4"}},"faf27d9f8b2b4c5e9a327fef6661e437":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fe1a5b7e379e4e5b9f97cfbb02512d3c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
